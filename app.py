import pandas as pd
import numpy as np
import json
import time
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import sqlite3
from pathlib import Path

# AI ç›¸é—œæ¨¡çµ„
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
import joblib

# Web ç›¸é—œæ¨¡çµ„
from flask import Flask, render_template, jsonify, request
from flask_socketio import SocketIO, emit
import threading

# Modbus é€šè¨Šæ¨¡çµ„
try:
    from pymodbus.client import ModbusTcpClient
    from pymodbus.exceptions import ModbusException
    MODBUS_AVAILABLE = True
except ImportError:
    MODBUS_AVAILABLE = False
    print("è­¦å‘Š: pymodbus æœªå®‰è£ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('oven_monitor.log', encoding='utf-8'),
        logging.StreamHandler()
    ])
logger = logging.getLogger(__name__)


class AlertLevel(Enum):
    """è­¦å ±ç­‰ç´š"""
    NORMAL = "æ­£å¸¸"
    WARNING = "è­¦å‘Š"
    CRITICAL = "åš´é‡"


@dataclass
class PLCMetric:
    """PLC æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    id: str
    name: str
    value: float
    unit: str
    timestamp: datetime
    status: AlertLevel = AlertLevel.NORMAL


@dataclass
class AnomalyResult:
    """ç•°å¸¸æª¢æ¸¬çµæœ"""
    timestamp: datetime
    is_anomaly: bool
    anomaly_score: float
    affected_metrics: List[str]
    description: str
    alert_level: AlertLevel


class DatabaseManager:
    """æ•¸æ“šåº«ç®¡ç†å™¨"""

    def __init__(self, db_path: str = "oven_monitoring.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•¸æ“šåº«"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # å‰µå»ºæŒ‡æ¨™æ•¸æ“šè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    metric_id TEXT NOT NULL,
                    metric_name TEXT NOT NULL,
                    value REAL NOT NULL,
                    unit TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    device_id INTEGER,
                    INDEX(metric_id),
                    INDEX(timestamp)
                )
            ''')

            # å‰µå»ºç•°å¸¸è¨˜éŒ„è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS anomalies (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    is_anomaly BOOLEAN NOT NULL,
                    anomaly_score REAL,
                    affected_metrics TEXT,
                    description TEXT,
                    alert_level TEXT,
                    resolved BOOLEAN DEFAULT FALSE
                )
            ''')

            # å‰µå»ºAIæ¨¡å‹è¨˜éŒ„è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS ai_models (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_name TEXT NOT NULL,
                    model_path TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    accuracy REAL,
                    is_active BOOLEAN DEFAULT FALSE
                )
            ''')

            conn.commit()

    def save_metrics(self, metrics: List[PLCMetric]):
        """ä¿å­˜æŒ‡æ¨™æ•¸æ“š"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            for metric in metrics:
                cursor.execute(
                    '''
                    INSERT INTO metrics (metric_id, metric_name, value, unit, timestamp)
                    VALUES (?, ?, ?, ?, ?)
                ''', (metric.id, metric.name, metric.value, metric.unit,
                      metric.timestamp))

            conn.commit()

    def save_anomaly(self, anomaly: AnomalyResult):
        """ä¿å­˜ç•°å¸¸è¨˜éŒ„"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            cursor.execute(
                '''
                INSERT INTO anomalies (timestamp, is_anomaly, anomaly_score, 
                                     affected_metrics, description, alert_level)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (anomaly.timestamp, anomaly.is_anomaly, anomaly.anomaly_score,
                  ','.join(anomaly.affected_metrics), anomaly.description,
                  anomaly.alert_level.value))

            conn.commit()

    def get_recent_metrics(self, hours: int = 24) -> pd.DataFrame:
        """ç²å–æœ€è¿‘çš„æŒ‡æ¨™æ•¸æ“š"""
        with sqlite3.connect(self.db_path) as conn:
            since_time = datetime.now() - timedelta(hours=hours)

            query = '''
                SELECT metric_id, metric_name, value, unit, timestamp
                FROM metrics 
                WHERE timestamp >= ?
                ORDER BY timestamp ASC
            '''

            return pd.read_sql_query(query, conn, params=[since_time])


class PLCDataReader:
    """PLC æ•¸æ“šè®€å–å™¨"""

    def __init__(self, config_path: str = "plc_points.json"):
        self.config = self.load_config(config_path)
        self.clients = {}
        self.last_read_time = {}

    def load_config(self, config_path: str) -> dict:
        """åŠ è¼‰ PLC é…ç½®"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨")
            return {"metric_groups": []}

    def connect_plc(self,
                    host: str,
                    port: int = 502) -> Optional[ModbusTcpClient]:
        """é€£æ¥ PLC"""
        if not MODBUS_AVAILABLE:
            return None

        try:
            client = ModbusTcpClient(host=host, port=port, timeout=10)
            if client.connect():
                logger.info(f"æˆåŠŸé€£æ¥åˆ° PLC: {host}:{port}")
                return client
            else:
                logger.error(f"ç„¡æ³•é€£æ¥åˆ° PLC: {host}:{port}")
                return None
        except Exception as e:
            logger.error(f"PLC é€£æ¥éŒ¯èª¤: {e}")
            return None

    def read_modbus_registers(self,
                              client: ModbusTcpClient,
                              start_address: int,
                              count: int,
                              device_id: int = 1) -> Optional[List[int]]:
        """è®€å– Modbus å¯„å­˜å™¨"""
        try:
            result = client.read_holding_registers(
                address=start_address - 1,  # Modbus åœ°å€å¾0é–‹å§‹
                count=count,
                slave=device_id)

            if result.isError():
                logger.error(f"Modbus è®€å–éŒ¯èª¤: {result}")
                return None

            return result.registers
        except Exception as e:
            logger.error(f"è®€å–å¯„å­˜å™¨éŒ¯èª¤: {e}")
            return None

    def parse_register_value(self, registers: List[int], offset: int,
                             data_type: str, scale_factor: float) -> float:
        """è§£æå¯„å­˜å™¨å€¼"""
        try:
            if data_type == "INT16":
                # è™•ç†æœ‰ç¬¦è™Ÿ16ä½æ•´æ•¸
                raw_value = registers[offset]
                if raw_value > 32767:
                    raw_value -= 65536
                return raw_value / scale_factor

            elif data_type == "FLOAT32":
                # è™•ç†32ä½æµ®é»æ•¸ï¼ˆéœ€è¦å…©å€‹å¯„å­˜å™¨ï¼‰
                if offset + 1 < len(registers):
                    # é«˜ä½åœ¨å‰ï¼Œä½ä½åœ¨å¾Œ
                    high_word = registers[offset]
                    low_word = registers[offset + 1]

                    # çµ„åˆæˆ32ä½æ•´æ•¸ï¼Œç„¶å¾Œè½‰æ›ç‚ºæµ®é»æ•¸
                    combined = (high_word << 16) | low_word
                    float_bytes = combined.to_bytes(4,
                                                    byteorder='big',
                                                    signed=False)
                    import struct
                    float_value = struct.unpack('>f', float_bytes)[0]
                    return float_value / scale_factor
                else:
                    logger.warning(f"FLOAT32 æ•¸æ“šä¸è¶³ï¼Œoffset: {offset}")
                    return 0.0

            else:
                logger.warning(f"ä¸æ”¯æ´çš„æ•¸æ“šé¡å‹: {data_type}")
                return 0.0

        except Exception as e:
            logger.error(f"è§£æå¯„å­˜å™¨å€¼éŒ¯èª¤: {e}")
            return 0.0

    def read_all_metrics(self,
                         plc_host: str = "192.168.1.100") -> List[PLCMetric]:
        """è®€å–æ‰€æœ‰æŒ‡æ¨™"""
        metrics = []
        current_time = datetime.now()

        for group in self.config.get("metric_groups", []):
            group_name = group["group_name"]
            device_id = group["device_id"]
            start_address = group["start_address"]
            count = group["count"]

            # ç²å–æˆ–å‰µå»º PLC å®¢æˆ¶ç«¯
            client_key = f"{plc_host}_{device_id}"
            if client_key not in self.clients:
                self.clients[client_key] = self.connect_plc(plc_host)

            client = self.clients[client_key]

            if client is None:
                # ä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
                logger.warning(f"ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šæ›¿ä»£ {group_name}")
                metrics.extend(self.generate_mock_data(group, current_time))
                continue

            # è®€å–å¯„å­˜å™¨æ•¸æ“š
            registers = self.read_modbus_registers(client, start_address,
                                                   count, device_id)

            if registers is None:
                logger.warning(f"ç„¡æ³•è®€å– {group_name} æ•¸æ“šï¼Œä½¿ç”¨æ¨¡æ“¬æ•¸æ“š")
                metrics.extend(self.generate_mock_data(group, current_time))
                continue

            # è§£ææ¯å€‹æŒ‡æ¨™
            for metric_config in group["metrics"]:
                try:
                    value = self.parse_register_value(
                        registers, metric_config["register_offset"],
                        metric_config["data_type"],
                        metric_config["scale_factor"])

                    metric = PLCMetric(id=metric_config["id"],
                                       name=metric_config["name"],
                                       value=value,
                                       unit=metric_config["unit"],
                                       timestamp=current_time)
                    metrics.append(metric)

                except Exception as e:
                    logger.error(f"è§£ææŒ‡æ¨™ {metric_config['id']} éŒ¯èª¤: {e}")

        logger.info(f"è®€å–äº† {len(metrics)} å€‹æŒ‡æ¨™æ•¸æ“š")
        return metrics

    def generate_mock_data(self, group_config: dict,
                           timestamp: datetime) -> List[PLCMetric]:
        """ç”Ÿæˆæ¨¡æ“¬æ•¸æ“š"""
        mock_metrics = []

        for metric_config in group_config["metrics"]:
            # æ ¹æ“šæŒ‡æ¨™é¡å‹ç”Ÿæˆåˆç†çš„æ¨¡æ“¬å€¼
            metric_id = metric_config["id"]

            if "temp" in metric_id:
                # æº«åº¦é¡æŒ‡æ¨™ï¼š150-200Â°C ç¯„åœ
                base_value = 175 + np.sin(time.time() / 300) * 15
                noise = np.random.normal(0, 2)
                value = max(100, min(250, base_value + noise))

            elif "humidity" in metric_id or "æ¿•åº¦" in metric_config["name"]:
                # æ¿•åº¦é¡æŒ‡æ¨™ï¼š30-60% ç¯„åœ
                base_value = 45 + np.sin(time.time() / 200) * 10
                noise = np.random.normal(0, 2)
                value = max(20, min(80, base_value + noise))

            elif "power" in metric_id or "åŠŸç‡" in metric_config["name"]:
                # åŠŸç‡é¡æŒ‡æ¨™ï¼š1500-3000W ç¯„åœ
                base_value = 2250 + np.sin(time.time() / 400) * 400
                noise = np.random.normal(0, 50)
                value = max(1000, min(4000, base_value + noise))

            elif "current" in metric_id or "é›»æµ" in metric_config["name"]:
                # é›»æµé¡æŒ‡æ¨™ï¼š10-50A ç¯„åœ
                base_value = 25 + np.sin(time.time() / 180) * 10
                noise = np.random.normal(0, 1)
                value = max(5, min(60, base_value + noise))

            elif "freq" in metric_id or "é »ç‡" in metric_config["name"]:
                # é »ç‡é¡æŒ‡æ¨™ï¼š40-60Hz ç¯„åœ
                base_value = 50 + np.sin(time.time() / 120) * 5
                noise = np.random.normal(0, 0.5)
                value = max(30, min(70, base_value + noise))

            elif "pressure" in metric_id or "å£“åŠ›" in metric_config["name"]:
                # å£“åŠ›é¡æŒ‡æ¨™ï¼šæ ¹æ“šå–®ä½è¨­å®šç¯„åœ
                if metric_config["unit"] == "kPa":
                    base_value = 500 + np.sin(time.time() / 250) * 100
                    noise = np.random.normal(0, 10)
                    value = max(200, min(800, base_value + noise))
                else:  # Pa
                    base_value = 100 + np.sin(time.time() / 150) * 20
                    noise = np.random.normal(0, 5)
                    value = max(50, min(200, base_value + noise))

            else:
                # å…¶ä»–æŒ‡æ¨™ï¼šé€šç”¨ç¯„åœ
                base_value = 100 + np.sin(time.time() / 300) * 20
                noise = np.random.normal(0, 5)
                value = max(50, min(150, base_value + noise))

            # å¶çˆ¾æ·»åŠ ç•°å¸¸å€¼ï¼ˆ5% æ©Ÿç‡ï¼‰
            if np.random.random() < 0.05:
                value *= np.random.choice([0.7, 1.4])  # ç•°å¸¸åé«˜æˆ–åä½

            metric = PLCMetric(id=metric_id,
                               name=metric_config["name"],
                               value=round(value, 2),
                               unit=metric_config["unit"],
                               timestamp=timestamp)
            mock_metrics.append(metric)

        return mock_metrics

    def close_connections(self):
        """é—œé–‰æ‰€æœ‰ PLC é€£æ¥"""
        for client in self.clients.values():
            if client:
                client.close()
        self.clients.clear()


class SmartOvenAI:
    """æ™ºæ…§çƒ˜ç®± AI åˆ†æå¼•æ“"""

    def __init__(self, model_path: str = "models/"):
        self.model_path = Path(model_path)
        self.model_path.mkdir(exist_ok=True)

        self.scaler = StandardScaler()
        self.anomaly_detector = IsolationForest(contamination=0.1,
                                                random_state=42,
                                                n_estimators=200)

        self.feature_columns = []
        self.is_trained = False
        self.training_history = []

    def prepare_features(self,
                         metrics_df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """æº–å‚™æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µ"""
        if metrics_df.empty:
            return None

        try:
            # è½‰æ›æ™‚é–“æˆ³æ ¼å¼
            if 'timestamp' in metrics_df.columns:
                metrics_df['timestamp'] = pd.to_datetime(
                    metrics_df['timestamp'])

            # é€è¦–è¡¨æ ¼å¼åŒ–
            pivot_df = metrics_df.pivot_table(index='timestamp',
                                              columns='metric_id',
                                              values='value',
                                              aggfunc='mean')

            # è™•ç†ç¼ºå¤±å€¼
            pivot_df = pivot_df.fillna(method='ffill').fillna(method='bfill')

            # æ·»åŠ æ™‚é–“ç‰¹å¾µ
            pivot_df['hour'] = pivot_df.index.hour
            pivot_df['day_of_week'] = pivot_df.index.dayofweek
            pivot_df['minute'] = pivot_df.index.minute

            # æ·»åŠ çµ±è¨ˆç‰¹å¾µ
            numeric_cols = pivot_df.select_dtypes(include=[np.number]).columns
            base_cols = [
                col for col in numeric_cols
                if col not in ['hour', 'day_of_week', 'minute']
            ]

            for col in base_cols:
                if len(pivot_df[col].dropna()) > 10:
                    # æ»¾å‹•çµ±è¨ˆ
                    pivot_df[f'{col}_ma5'] = pivot_df[col].rolling(
                        5, min_periods=1).mean()
                    pivot_df[f'{col}_std5'] = pivot_df[col].rolling(
                        5, min_periods=1).std()
                    pivot_df[f'{col}_diff'] = pivot_df[col].diff()

                    # ç•°å¸¸ç¨‹åº¦ç‰¹å¾µ
                    mean_val = pivot_df[col].mean()
                    std_val = pivot_df[col].std()
                    if std_val > 0:
                        pivot_df[f'{col}_zscore'] = (pivot_df[col] -
                                                     mean_val) / std_val

            # æ¸…ç†æ•¸æ“š
            cleaned_df = pivot_df.dropna()

            if cleaned_df.empty:
                logger.warning("ç‰¹å¾µæº–å‚™å¾Œæ²’æœ‰å¯ç”¨æ•¸æ“š")
                return None

            return cleaned_df

        except Exception as e:
            logger.error(f"æº–å‚™ç‰¹å¾µæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def train_model(self, features_df: pd.DataFrame) -> bool:
        """è¨“ç·´ AI æ¨¡å‹"""
        if features_df is None or features_df.empty:
            return False

        try:
            # é¸æ“‡æ•¸å€¼ç‰¹å¾µ
            numeric_features = features_df.select_dtypes(include=[np.number])

            # ç§»é™¤å¸¸æ•¸åˆ—
            valid_cols = []
            for col in numeric_features.columns:
                if numeric_features[col].std() > 1e-8:
                    valid_cols.append(col)

            if len(valid_cols) < 2:
                logger.warning("æœ‰æ•ˆç‰¹å¾µæ•¸é‡ä¸è¶³")
                return False

            numeric_features = numeric_features[valid_cols]
            self.feature_columns = valid_cols

            # æ¨™æº–åŒ–
            scaled_data = self.scaler.fit_transform(numeric_features)

            # è¨“ç·´ç•°å¸¸æª¢æ¸¬æ¨¡å‹
            self.anomaly_detector.fit(scaled_data)

            # ä¿å­˜æ¨¡å‹
            self.save_model()

            self.is_trained = True

            # è¨˜éŒ„è¨“ç·´æ­·å²
            training_info = {
                'timestamp': datetime.now(),
                'features_count': len(valid_cols),
                'samples_count': len(numeric_features),
                'feature_names': valid_cols
            }
            self.training_history.append(training_info)

            logger.info(
                f"AI æ¨¡å‹è¨“ç·´å®Œæˆï¼Œä½¿ç”¨ {len(valid_cols)} å€‹ç‰¹å¾µï¼Œ{len(numeric_features)} å€‹æ¨£æœ¬"
            )
            return True

        except Exception as e:
            logger.error(f"æ¨¡å‹è¨“ç·´éŒ¯èª¤: {e}")
            return False

    def predict_anomalies(self,
                          features_df: pd.DataFrame) -> List[AnomalyResult]:
        """é æ¸¬ç•°å¸¸"""
        if not self.is_trained or features_df is None or features_df.empty:
            return []

        try:
            # é¸æ“‡ç›¸åŒçš„ç‰¹å¾µ
            if not all(col in features_df.columns
                       for col in self.feature_columns):
                logger.warning("ç‰¹å¾µä¸åŒ¹é…ï¼Œç„¡æ³•é€²è¡Œé æ¸¬")
                return []

            numeric_features = features_df[self.feature_columns]
            scaled_data = self.scaler.transform(numeric_features)

            # é æ¸¬ç•°å¸¸
            predictions = self.anomaly_detector.predict(scaled_data)
            scores = self.anomaly_detector.decision_function(scaled_data)

            results = []
            for i, (timestamp, is_anomaly, score) in enumerate(
                    zip(features_df.index, predictions == -1, scores)):
                if is_anomaly:
                    # æ‰¾å‡ºæœ€ç•°å¸¸çš„æŒ‡æ¨™
                    row_data = numeric_features.iloc[i]
                    affected_metrics = self.identify_anomalous_metrics(
                        row_data)

                    # ç¢ºå®šè­¦å ±ç­‰ç´š
                    if score < -0.5:
                        alert_level = AlertLevel.CRITICAL
                    elif score < -0.2:
                        alert_level = AlertLevel.WARNING
                    else:
                        alert_level = AlertLevel.NORMAL

                    description = f"æª¢æ¸¬åˆ°ç•°å¸¸æ¨¡å¼ï¼Œç•°å¸¸åˆ†æ•¸: {score:.3f}"
                    if affected_metrics:
                        description += f"ï¼Œä¸»è¦ç•°å¸¸æŒ‡æ¨™: {', '.join(affected_metrics[:3])}"

                    result = AnomalyResult(timestamp=timestamp,
                                           is_anomaly=is_anomaly,
                                           anomaly_score=score,
                                           affected_metrics=affected_metrics,
                                           description=description,
                                           alert_level=alert_level)
                    results.append(result)

            logger.info(f"ç•°å¸¸æª¢æ¸¬å®Œæˆï¼Œç™¼ç¾ {len(results)} å€‹ç•°å¸¸")
            return results

        except Exception as e:
            logger.error(f"ç•°å¸¸é æ¸¬éŒ¯èª¤: {e}")
            return []

    def identify_anomalous_metrics(self, row_data: pd.Series) -> List[str]:
        """è­˜åˆ¥ç•°å¸¸æŒ‡æ¨™"""
        anomalous_metrics = []

        try:
            # è¨ˆç®— Z-score
            for col in row_data.index:
                if col.endswith('_zscore'):
                    original_col = col.replace('_zscore', '')
                    z_score = abs(row_data[col])

                    if z_score > 2.5:  # Z-score é–¾å€¼
                        anomalous_metrics.append(original_col)

            # å¦‚æœæ²’æœ‰ Z-score ç‰¹å¾µï¼Œä½¿ç”¨å…¶ä»–æ–¹æ³•
            if not anomalous_metrics:
                # å°‹æ‰¾æœ€å¤§åå·®çš„æŒ‡æ¨™
                base_cols = [
                    col for col in row_data.index if not any(
                        suffix in col
                        for suffix in ['_ma5', '_std5', '_diff', '_zscore'])
                ]

                # ç°¡å–®çš„åå·®æª¢æ¸¬
                for col in base_cols[:5]:  # åªæª¢æŸ¥å‰5å€‹ä¸»è¦æŒ‡æ¨™
                    anomalous_metrics.append(col)

        except Exception as e:
            logger.error(f"è­˜åˆ¥ç•°å¸¸æŒ‡æ¨™éŒ¯èª¤: {e}")

        return anomalous_metrics[:5]  # æœ€å¤šè¿”å›5å€‹ç•°å¸¸æŒ‡æ¨™

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_data = {
                'scaler': self.scaler,
                'anomaly_detector': self.anomaly_detector,
                'feature_columns': self.feature_columns,
                'training_time': datetime.now()
            }

            model_file = self.model_path / f"oven_ai_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
            joblib.dump(model_data, model_file)
            logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_file}")

        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹éŒ¯èª¤: {e}")

    def load_model(self, model_file: str):
        """åŠ è¼‰æ¨¡å‹"""
        try:
            model_data = joblib.load(model_file)
            self.scaler = model_data['scaler']
            self.anomaly_detector = model_data['anomaly_detector']
            self.feature_columns = model_data['feature_columns']
            self.is_trained = True

            logger.info(f"æ¨¡å‹å·²å¾ {model_file} åŠ è¼‰")
            return True

        except Exception as e:
            logger.error(f"åŠ è¼‰æ¨¡å‹éŒ¯èª¤: {e}")
            return False


class SmartOvenMonitor:
    """æ™ºæ…§çƒ˜ç®±ç›£æ§ç³»çµ±ä¸»æ§åˆ¶å™¨"""

    def __init__(self,
                 plc_host: str = "192.168.1.100",
                 config_path: str = "plc_points.json"):
        self.plc_host = plc_host
        self.db_manager = DatabaseManager()
        self.plc_reader = PLCDataReader(config_path)
        self.ai_engine = SmartOvenAI()

        self.is_running = False
        self.monitoring_thread = None
        self.current_metrics = []
        self.recent_anomalies = []

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            'total_readings': 0,
            'anomaly_count': 0,
            'last_update': None,
            'system_status': 'STARTING'
        }

    def start_monitoring(self, interval: int = 30):
        """é–‹å§‹ç›£æ§"""
        if self.is_running:
            logger.warning("ç›£æ§ç³»çµ±å·²åœ¨é‹è¡Œä¸­")
            return

        self.is_running = True
        self.stats['system_status'] = 'RUNNING'

        self.monitoring_thread = threading.Thread(target=self._monitoring_loop,
                                                  args=(interval, ),
                                                  daemon=True)
        self.monitoring_thread.start()
        logger.info(f"ç›£æ§ç³»çµ±å·²å•Ÿå‹•ï¼Œæ¡æ¨£é–“éš”: {interval} ç§’")

    def stop_monitoring(self):
        """åœæ­¢ç›£æ§"""
        self.is_running = False
        self.stats['system_status'] = 'STOPPED'

        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)

        self.plc_reader.close_connections()
        logger.info("ç›£æ§ç³»çµ±å·²åœæ­¢")

    def _monitoring_loop(self, interval: int):
        """ç›£æ§ä¸»å¾ªç’°"""
        logger.info("é–‹å§‹ç›£æ§å¾ªç’°")

        # é¦–æ¬¡è¨“ç·´æ¨¡å‹
        self._initial_training()

        while self.is_running:
            try:
                # è®€å– PLC æ•¸æ“š
                metrics = self.plc_reader.read_all_metrics(self.plc_host)

                if metrics:
                    # ä¿å­˜åˆ°æ•¸æ“šåº«
                    self.db_manager.save_metrics(metrics)
                    self.current_metrics = metrics

                    # ç•°å¸¸æª¢æ¸¬
                    self._perform_anomaly_detection()

                    # æ›´æ–°çµ±è¨ˆ
                    self.stats['total_readings'] += len(metrics)
                    self.stats['last_update'] = datetime.now()

                    logger.info(f"æˆåŠŸè™•ç† {len(metrics)} å€‹æŒ‡æ¨™æ•¸æ“š")

                time.sleep(interval)

            except Exception as e:
                logger.error(f"ç›£æ§å¾ªç’°éŒ¯èª¤: {e}")
                time.sleep(interval)

    def _initial_training(self):
        """åˆå§‹æ¨¡å‹è¨“ç·´"""
        try:
            # ç²å–æ­·å²æ•¸æ“š
            historical_data = self.db_manager.get_recent_metrics(hours=48)

            if len(historical_data) < 100:
                logger.info("æ­·å²æ•¸æ“šä¸è¶³ï¼Œå°‡åœ¨é‹è¡Œéç¨‹ä¸­ç´¯ç©æ•¸æ“šé€²è¡Œè¨“ç·´")
                return

            # æº–å‚™ç‰¹å¾µä¸¦è¨“ç·´æ¨¡å‹
            features_df = self.ai_engine.prepare_features(historical_data)
            if features_df is not None:
                success = self.ai_engine.train_model(features_df)
                if success:
                    logger.info("AI æ¨¡å‹åˆå§‹è¨“ç·´å®Œæˆ")
                else:
                    logger.warning("AI æ¨¡å‹åˆå§‹è¨“ç·´å¤±æ•—")

        except Exception as e:
            logger.error(f"åˆå§‹è¨“ç·´éŒ¯èª¤: {e}")

    def _perform_anomaly_detection(self):
        """åŸ·è¡Œç•°å¸¸æª¢æ¸¬"""
        try:
            if not self.ai_engine.is_trained:
                # å˜—è©¦é‡æ–°è¨“ç·´
                recent_data = self.db_manager.get_recent_metrics(hours=2)
                if len(recent_data) >= 50:
                    features_df = self.ai_engine.prepare_features(recent_data)
                    if features_df is not None:
                        self.ai_engine.train_model(features_df)
                return

            # ç²å–æœ€è¿‘æ•¸æ“šé€²è¡Œç•°å¸¸æª¢æ¸¬
            recent_data = self.db_manager.get_recent_metrics(hours=1)
            if len(recent_data) < 10:
                return

            features_df = self.ai_engine.prepare_features(recent_data)
            if features_df is not None:
                anomalies = self.ai_engine.predict_anomalies(features_df)

                for anomaly in anomalies:
                    self.db_manager.save_anomaly(anomaly)
                    self.recent_anomalies.append(anomaly)
                    self.stats['anomaly_count'] += 1

                    # è¨˜éŒ„ç•°å¸¸
                    logger.warning(f"æª¢æ¸¬åˆ°ç•°å¸¸: {anomaly.description}")

                # ä¿æŒæœ€è¿‘100å€‹ç•°å¸¸è¨˜éŒ„
                if len(self.recent_anomalies) > 100:
                    self.recent_anomalies = self.recent_anomalies[-100:]

        except Exception as e:
            logger.error(f"ç•°å¸¸æª¢æ¸¬éŒ¯èª¤: {e}")

    def get_current_status(self) -> Dict:
        """ç²å–ç•¶å‰ç³»çµ±ç‹€æ…‹"""
        try:
            # è¨ˆç®—é—œéµæŒ‡æ¨™
            temp_metrics = [
                m for m in self.current_metrics if 'temp' in m.id.lower()
            ]
            power_metrics = [
                m for m in self.current_metrics if 'power' in m.id.lower()
            ]

            avg_temp = np.mean([m.value
                                for m in temp_metrics]) if temp_metrics else 0
            total_power = sum([m.value
                               for m in power_metrics]) if power_metrics else 0

            # è¨ˆç®—æ•ˆç‡åˆ†æ•¸ï¼ˆç°¡åŒ–ç®—æ³•ï¼‰
            efficiency_score = min(
                100, max(0, 100 - (len(self.recent_anomalies) * 2)))

            # æœ€è¿‘ç•°å¸¸æ•¸é‡
            recent_anomalies_24h = len([
                a for a in self.recent_anomalies
                if (datetime.now() - a.timestamp).total_seconds() < 86400
            ])

            return {
                'system_status':
                self.stats['system_status'],
                'last_update':
                self.stats['last_update'].isoformat()
                if self.stats['last_update'] else None,
                'total_readings':
                self.stats['total_readings'],
                'current_temp':
                round(avg_temp, 1),
                'current_power':
                round(total_power, 0),
                'efficiency_score':
                round(efficiency_score, 1),
                'anomaly_count_24h':
                recent_anomalies_24h,
                'ai_model_trained':
                self.ai_engine.is_trained,
                'active_metrics_count':
                len(self.current_metrics),
                'recent_anomalies': [
                    {
                        'timestamp': a.timestamp.isoformat(),
                        'description': a.description,
                        'alert_level': a.alert_level.value,
                        'affected_metrics': a.affected_metrics[:3]  # åªé¡¯ç¤ºå‰3å€‹
                    } for a in self.recent_anomalies[-5:]  # æœ€è¿‘5å€‹ç•°å¸¸
                ]
            }
        except Exception as e:
            logger.error(f"ç²å–ç³»çµ±ç‹€æ…‹éŒ¯èª¤: {e}")
            return {'error': str(e)}

    def get_metrics_data(self, hours: int = 1) -> Dict:
        """ç²å–æŒ‡æ¨™æ•¸æ“š"""
        try:
            df = self.db_manager.get_recent_metrics(hours)

            if df.empty:
                return {'timestamps': [], 'metrics': {}}

            # æŒ‰æ™‚é–“æ’åº
            df = df.sort_values('timestamp')

            # è½‰æ›ç‚ºå‰ç«¯éœ€è¦çš„æ ¼å¼
            timestamps = df['timestamp'].dt.strftime('%H:%M:%S').tolist()

            metrics_data = {}
            for metric_id in df['metric_id'].unique():
                metric_df = df[df['metric_id'] == metric_id]
                metrics_data[metric_id] = {
                    'name': metric_df['metric_name'].iloc[0],
                    'unit': metric_df['unit'].iloc[0],
                    'values': metric_df['value'].tolist()
                }

            return {'timestamps': timestamps, 'metrics': metrics_data}

        except Exception as e:
            logger.error(f"ç²å–æŒ‡æ¨™æ•¸æ“šéŒ¯èª¤: {e}")
            return {'error': str(e)}


# Flask Web æ‡‰ç”¨
app = Flask(__name__)
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨å±€ç›£æ§ç³»çµ±å¯¦ä¾‹
monitor = None


@app.route('/')
def index():
    """ä¸»é """
    return render_template('smart_oven_dashboard.html')


@app.route('/api/status')
def api_status():
    """ç²å–ç³»çµ±ç‹€æ…‹ API"""
    if monitor:
        return jsonify(monitor.get_current_status())
    else:
        return jsonify({'error': 'ç›£æ§ç³»çµ±æœªå•Ÿå‹•'}), 500


@app.route('/api/metrics')
def api_metrics():
    """ç²å–æŒ‡æ¨™æ•¸æ“š API"""
    hours = request.args.get('hours', 1, type=int)
    if monitor:
        return jsonify(monitor.get_metrics_data(hours))
    else:
        return jsonify({'error': 'ç›£æ§ç³»çµ±æœªå•Ÿå‹•'}), 500


@app.route('/api/start', methods=['POST'])
def api_start():
    """å•Ÿå‹•ç›£æ§ API"""
    global monitor
    try:
        if monitor is None:
            data = request.get_json() or {}
            plc_host = data.get('plc_host', '192.168.1.100')
            interval = data.get('interval', 30)

            monitor = SmartOvenMonitor(plc_host=plc_host)
            monitor.start_monitoring(interval)

            return jsonify({'message': 'ç›£æ§ç³»çµ±å·²å•Ÿå‹•', 'status': 'success'})
        else:
            return jsonify({'message': 'ç›£æ§ç³»çµ±å·²åœ¨é‹è¡Œä¸­', 'status': 'warning'})
    except Exception as e:
        logger.error(f"å•Ÿå‹•ç›£æ§éŒ¯èª¤: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/stop', methods=['POST'])
def api_stop():
    """åœæ­¢ç›£æ§ API"""
    global monitor
    try:
        if monitor:
            monitor.stop_monitoring()
            monitor = None
            return jsonify({'message': 'ç›£æ§ç³»çµ±å·²åœæ­¢', 'status': 'success'})
        else:
            return jsonify({'message': 'ç›£æ§ç³»çµ±æœªé‹è¡Œ', 'status': 'warning'})
    except Exception as e:
        logger.error(f"åœæ­¢ç›£æ§éŒ¯èª¤: {e}")
        return jsonify({'error': str(e)}), 500


@socketio.on('connect')
def handle_connect():
    """WebSocket é€£æ¥è™•ç†"""
    logger.info('WebSocket å®¢æˆ¶ç«¯å·²é€£æ¥')
    if monitor:
        emit('status_update', monitor.get_current_status())


@socketio.on('disconnect')
def handle_disconnect():
    """WebSocket æ–·é–‹è™•ç†"""
    logger.info('WebSocket å®¢æˆ¶ç«¯å·²æ–·é–‹')


def broadcast_updates():
    """å»£æ’­æ›´æ–°æ•¸æ“š"""
    while True:
        try:
            if monitor and monitor.is_running:
                status = monitor.get_current_status()
                socketio.emit('status_update', status)

                # å¦‚æœæœ‰æ–°ç•°å¸¸ï¼Œç™¼é€è­¦å ±
                if status.get('recent_anomalies'):
                    latest_anomaly = status['recent_anomalies'][-1]
                    if latest_anomaly.get('alert_level') in ['è­¦å‘Š', 'åš´é‡']:
                        socketio.emit('anomaly_alert', latest_anomaly)

            time.sleep(10)  # æ¯10ç§’å»£æ’­ä¸€æ¬¡

        except Exception as e:
            logger.error(f"å»£æ’­æ›´æ–°éŒ¯èª¤: {e}")
            time.sleep(10)


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("=== æ™ºæ…§çƒ˜ç®± AI ç›£æ§ç³»çµ± ===")
    print("ä½œè€…: AI Assistant")
    print("ç‰ˆæœ¬: 1.0.0")
    print("=" * 40)

    # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
    config_file = Path("plc_points.json")
    if not config_file.exists():
        logger.warning("PLC é…ç½®æª”æ¡ˆä¸å­˜åœ¨ï¼Œå°‡ä½¿ç”¨é è¨­é…ç½®")

    # å•Ÿå‹•å»£æ’­ç·šç¨‹
    broadcast_thread = threading.Thread(target=broadcast_updates, daemon=True)
    broadcast_thread.start()

    try:
        # è‡ªå‹•å•Ÿå‹•ç›£æ§ï¼ˆå¯é¸ï¼‰
        global monitor
        monitor = SmartOvenMonitor()
        monitor.start_monitoring(interval=30)

        logger.info("Web æœå‹™å™¨å•Ÿå‹•ä¸­...")
        print("\nğŸŒ Web ç•Œé¢åœ°å€: http://localhost:5000")
        print("ğŸ“Š API æ–‡æª”: http://localhost:5000/api/status")
        print("ğŸ”§ æ§åˆ¶é¢æ¿: http://localhost:5000")
        print("\næŒ‰ Ctrl+C åœæ­¢æœå‹™")

        # å•Ÿå‹• Flask æ‡‰ç”¨
        socketio.run(app, host='0.0.0.0', port=5000, debug=False)

    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°åœæ­¢ä¿¡è™Ÿ")
    except Exception as e:
        logger.error(f"ç³»çµ±éŒ¯èª¤: {e}")
    finally:
        if monitor:
            monitor.stop_monitoring()
        print("\nâœ… ç³»çµ±å·²å®‰å…¨é—œé–‰")


if __name__ == "__main__":
    main()
